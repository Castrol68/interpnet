\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{kastner1999increased}
\citation{lamme1998feedforward}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{bengio2013representation}
\citation{WahCUB_200_2011}
\citation{reed2016learning}
\citation{gao2016compact}
\@writefile{toc}{\contentsline {section}{\numberline {2}Approach}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problem Statement}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}InterpNET Architecture}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Model Architecture for CUB Dataset}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {paragraph}{CUB Dataset}{2}{section*.1}}
\citation{donahue2015long}
\citation{kingma2014adam}
\citation{bleu}
\citation{evaluating_bleu}
\citation{denkowski:lavie:meteor-wmt:2014}
\citation{vedantam2015cider}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Model. First, the network extracts 8,192 features using a pre-trained bilinear compact pooling network. Then, it classifies the category of bird using a fully connected network. It then concatenates the internal activations of the fully connected network and provides them as input to a LRCN$_{2f}$ language-generating RNN which is unrolled to produce an explanation of the classification.}}{3}{figure.1}}
\newlabel{fig:model}{{1}{3}{The Model. First, the network extracts 8,192 features using a pre-trained bilinear compact pooling network. Then, it classifies the category of bird using a fully connected network. It then concatenates the internal activations of the fully connected network and provides them as input to a LRCN$_{2f}$ language-generating RNN which is unrolled to produce an explanation of the classification}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Training Procedure}{3}{subsection.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Evaluation Metrics}{3}{subsection.3.1}}
\citation{hendricks2016generating}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results. Explanation metrics and Classification Accuracy for a variety of models. InterpNET$_2$ achieves the highest metrics, except for classification accuracy. Higher is better for all metrics.}}{4}{table.1}}
\newlabel{tab:results}{{1}{4}{Results. Explanation metrics and Classification Accuracy for a variety of models. InterpNET$_2$ achieves the highest metrics, except for classification accuracy. Higher is better for all metrics}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experiments Setup}{4}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Quantitative Results}{4}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{4}{section.4}}
\bibdata{mybib}
\bibcite{kastner1999increased}{1}
\bibcite{lamme1998feedforward}{2}
\bibcite{bengio2013representation}{3}
\bibcite{WahCUB_200_2011}{4}
\bibcite{reed2016learning}{5}
\bibcite{gao2016compact}{6}
\bibcite{donahue2015long}{7}
\bibcite{kingma2014adam}{8}
\bibcite{bleu}{9}
\bibcite{evaluating_bleu}{10}
\bibcite{denkowski:lavie:meteor-wmt:2014}{11}
\bibcite{vedantam2015cider}{12}
\bibcite{hendricks2016generating}{13}
\bibcite{russakovsky2015imagenet}{14}
\bibcite{rowley1998neural}{15}
\bibcite{bahdanau2014neural}{16}
\bibcite{rush2015neural}{17}
\bibcite{showandtell2014}{18}
\bibcite{antol2015vqa}{19}
\bibstyle{unsrt}
\citation{russakovsky2015imagenet}
\citation{rowley1998neural}
\citation{bahdanau2014neural}
\citation{rush2015neural}
\citation{showandtell2014}
\citation{antol2015vqa}
\citation{hendricks2016generating}
\citation{hendricks2016generating}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example classifications and explanations. Green and red text signify a valid and invalid descriptor respectively.}}{6}{figure.2}}
\newlabel{fig:examples}{{2}{6}{Example classifications and explanations. Green and red text signify a valid and invalid descriptor respectively}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Appendix}{6}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Related Work}{6}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Qualitative Results}{6}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Training Procedure}{6}{subsection.5.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces InterpNET Training Procedure.}}{7}{algorithm.1}}
\newlabel{alg2}{{1}{7}{Training Procedure}{algorithm.1}{}}
